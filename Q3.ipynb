{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nxqp6lgF5Vg8"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\n",
        "\n",
        "from IPython import display\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "u30Y_J0H5fds"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    pad_token='<pad>',\n",
        "    unk_token='<unk>'\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens({\n",
        "    \"bos_token\": '</s>',\n",
        "    \"eos_token\": '</s>',\n",
        "    \"pad_token\": '<pad>',\n",
        "    \"unk_token\": '<unk>'\n",
        "})\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0],\n",
        "    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0],\n",
        "    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/gpt2/\")\n",
        "config.save_pretrained(\"/content/gpt2/\")\n",
        "\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n",
        "!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\" -P /content/gpt2/"
      ],
      "metadata": {
        "id": "69tIzftQ5gED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b52c85-972b-4bf2-df1d-c3dc127bc69e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-21 16:55:24--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.17, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1706115324&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjExNTMyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=hY3PR3ARgQZGepO6VCVYhom50E5KckPV%7EJSani2S76yYUrcRNubGia46otOnyV2NnTC9VgSG4Y0FKCsxb-KtxEfmtzbTDqJHg4XUQMbHaNg4iLWQ1QRJL1SoTDulb1KXZ1lm3H5jhtwYex2jDilanhL-0ydkPY38b2zCqAQHDwaxKjYijZ2nBrnrV2IUw9b8P0Syyhmdk6GET04Dy-SJnT2L1dht-nSikZmkFpLeA99wwbfTc91FreTGsxxk354WvjAzSSkfbVjamJot0EyN79-ZUtPQE9tcQwVEAF1piEiLHbIr%7E04LQWyGjzDpGzNmFN%7EQew4iW90jNHIbduFHgQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-01-21 16:55:24--  https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1706115324&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjExNTMyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=hY3PR3ARgQZGepO6VCVYhom50E5KckPV%7EJSani2S76yYUrcRNubGia46otOnyV2NnTC9VgSG4Y0FKCsxb-KtxEfmtzbTDqJHg4XUQMbHaNg4iLWQ1QRJL1SoTDulb1KXZ1lm3H5jhtwYex2jDilanhL-0ydkPY38b2zCqAQHDwaxKjYijZ2nBrnrV2IUw9b8P0Syyhmdk6GET04Dy-SJnT2L1dht-nSikZmkFpLeA99wwbfTc91FreTGsxxk354WvjAzSSkfbVjamJot0EyN79-ZUtPQE9tcQwVEAF1piEiLHbIr%7E04LQWyGjzDpGzNmFN%7EQew4iW90jNHIbduFHgQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.163.125.79, 3.163.125.41, 3.163.125.12, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.163.125.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 485044198 (463M) [application/octet-stream]\n",
            "Saving to: ‘/content/gpt2/pytorch_model.bin.2’\n",
            "\n",
            "pytorch_model.bin.2 100%[===================>] 462.57M  69.7MB/s    in 6.0s    \n",
            "\n",
            "2024-01-21 16:55:30 (76.6 MB/s) - ‘/content/gpt2/pytorch_model.bin.2’ saved [485044198/485044198]\n",
            "\n",
            "--2024-01-21 16:55:30--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.17, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748949 (2.6M) [text/plain]\n",
            "Saving to: ‘/content/gpt2/tokenizer.json.3’\n",
            "\n",
            "tokenizer.json.3    100%[===================>]   2.62M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-01-21 16:55:31 (39.5 MB/s) - ‘/content/gpt2/tokenizer.json.3’ saved [2748949/2748949]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  def __init__(self, data_path, tokenizer, max_length=16):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    with open(data_path, \"r\") as file:\n",
        "      lines = file.readlines()\n",
        "    self.lines = lines[2:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "      input_poem = self.lines[idx].removesuffix(\"\\n\")\n",
        "      input = self.tokenizer('<s>' + input_poem + '</s>',\n",
        "                             max_length=self.max_length,\n",
        "                             truncation=True,\n",
        "                             padding='max_length',\n",
        "                             return_tensors='pt')\n",
        "      return input['input_ids'], input['attention_mask']"
      ],
      "metadata": {
        "id": "XGgK2Wq67vtW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ferdousi.txt', 'r', encoding='utf-8') as f:\n",
        "    texts = [line.strip() for line in f.readlines()]"
      ],
      "metadata": {
        "id": "zo9gGxER7hyD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "dataset = Dataset('ferdousi.txt', tokenizer, max_length=16)\n",
        "\n",
        "train_ratio = 0.7\n",
        "test_ratio = 0.3\n",
        "\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_indices = range(0, train_size)\n",
        "test_indices = range(train_size , len(dataset))\n",
        "\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "test_dataset = Subset(dataset, test_indices)"
      ],
      "metadata": {
        "id": "Qq9DzfE88YZD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "M3klj3-J9T9O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "mWeV7ny2Motf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "seed_val = 49\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "MHoX6tHq9bCj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f3GPRxR9vBT",
        "outputId": "30d396cf-bf3f-4ce5-9c82-8ebb92d1ad40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "warmup_steps = 1e2\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "MaMdhzJa90jT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    with tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch', leave=False) as epoch_progress:\n",
        "        for _ , batch in enumerate(epoch_progress):\n",
        "\n",
        "            inputs = batch[0].to(device)\n",
        "            masks = batch[1].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids=inputs.squeeze(dim=1), labels=inputs.squeeze(dim=1), attention_mask=masks.squeeze(dim=1))\n",
        "            loss = outputs.loss\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        perplexity = torch.exp(torch.tensor(avg_train_loss, device=device))\n",
        "\n",
        "        print()\n",
        "        print(f'Average Training Loss: {avg_train_loss}. Perplexity Loss of Training {perplexity}')\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ADMywn-f7D",
        "outputId": "9402c61b-c2d7-44ed-a966-3d7444bd8713"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 2.104804662509163. Perplexity Loss of Training 8.205500602722168\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 1.849963004826346. Perplexity Loss of Training 6.359583854675293\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 1.641116786465835. Perplexity Loss of Training 5.160930156707764\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 1.43403081158234. Perplexity Loss of Training 4.1955766677856445\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 1.2391846383930598. Perplexity Loss of Training 3.4527971744537354\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 1.074660072449412. Perplexity Loss of Training 2.928997039794922\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 0.9467360862277395. Perplexity Loss of Training 2.5772838592529297\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 0.8467941443660901. Perplexity Loss of Training 2.332158327102661\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 0.7738240505522557. Perplexity Loss of Training 2.1680409908294678\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 0.8539439885332353. Perplexity Loss of Training 2.3488924503326416\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt,model,tokenizer):\n",
        "    result = tokenizer(text='<s>' + prompt + '</s>',return_tensors='pt').to(device)\n",
        "    model.eval()\n",
        "    temp=len(prompt)\n",
        "    output_ids = model.generate(result.input_ids,\n",
        "                                attention_mask=result.attention_mask,\n",
        "                                min_length=int(temp/2),\n",
        "                                max_length=temp-2,\n",
        "                                no_repeat_ngram_size=1,\n",
        "                                num_beams=50)\n",
        "    poem = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return poem[temp:]\n"
      ],
      "metadata": {
        "id": "nnDZrKpebyf-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'که برداشت  او دیپ با فاطمی'\n",
        "generated_poem = generate(prompt,model, tokenizer)\n",
        "print(\"Prompt Poem:\")\n",
        "print(prompt)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "id": "Uf_bDcQzb0Pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430d8467-d56c-4336-9fdb-f5e9906ae41c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Poem:\n",
            "که برداشت  او دیپ با فاطمی\n",
            "Generated Poem:\n",
            " غنیمتاردند ستردان و چکاووش مرد سنگ اکوان\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'چه آمد برویش که ما را بخواست'\n",
        "generated_poem = generate(prompt,model, tokenizer)\n",
        "print(\"Prompt Poem:\")\n",
        "print(prompt)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbg_lhU76c1j",
        "outputId": "8349e1d0-ae41-4dac-de83-c5041d04756d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Poem:\n",
            "چه آمد برویش که ما را بخواست\n",
            "Generated Poem:\n",
            " غنیمت ستمکقون به بزد مهان چو شیر مرد دلیر\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'نهادیم بر چرخ گردنده زین'\n",
        "generated_poem = generate(prompt,model, tokenizer)\n",
        "print(\"Prompt Poem:\")\n",
        "print(prompt)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fwbKIj6c7S",
        "outputId": "2aff4cb8-e2fc-478a-a9d2-064db3636bb6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Poem:\n",
            "نهادیم بر چرخ گردنده زین\n",
            "Generated Poem:\n",
            "گذلیجغ ویسه به هر سه بدخواهش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'چو بار درخت وفا را بدید'\n",
        "generated_poem = generate(prompt,model, tokenizer)\n",
        "print(\"Prompt Poem:\")\n",
        "print(prompt)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeX7OJsx6dBd",
        "outputId": "ec3ef817-0e54-4036-8c6f-e73eb0855011"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Poem:\n",
            "چو بار درخت وفا را بدید\n",
            "Generated Poem:\n",
            "ذ،ج ستو و گفت زهش به ایوان نو\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "with tqdm(test_dataloader, unit='batch', leave=False) as epoch_progress:\n",
        "    for _ , batch in enumerate(epoch_progress):\n",
        "\n",
        "        inputs = batch[0].to(device)\n",
        "        masks = batch[1].to(device)\n",
        "\n",
        "        outputs = model(input_ids=inputs.squeeze(dim=1), labels=inputs.squeeze(dim=1), attention_mask=masks.squeeze(dim=1))\n",
        "        loss = outputs.loss\n",
        "        batch_loss = loss.item()\n",
        "        test_loss += batch_loss\n",
        "\n",
        "    avg_loss = test_loss / len(test_dataloader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss, device=device))\n",
        "    print(f'Perplexity of Test Data: {perplexity}.')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovh5MAVE3aW7",
        "outputId": "ba70c824-a805-4805-9af7-50517ada12f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Test Data: 24.899248123168945.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}